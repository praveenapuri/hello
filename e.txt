# Region youâ€™re using for the dashboard
variable "region" {
  type    = string
  default = "us-east-1"
}

# Target capacity to compute Storage % (EFS is elastic; pick your planning size)
variable "efs_target_capacity_bytes" {
  type    = number
  default = 10995116277760  # 10 TiB
}

# Get EFS IDs using /bin/sh and AWS CLI (no jq required)
data "external" "efs_list" {
  program = [
    "/bin/sh",
    "-lc",
    "aws efs describe-file-systems --region ${var.region} --query 'FileSystems[].FileSystemId' --output json"
  ]
}

locals {
  # external provider returns the JSON directly as 'result'
  efs_ids         = try(data.external.efs_list.result, [])
  efs_id_to_token = { for id in local.efs_ids : id => replace(id, "-", "") }

  # Build per-filesystem gauge series (same math as before)
  efs_gauge_metrics = flatten([
    for fs_id in local.efs_ids : [
      [ "AWS/EFS", "MeteredIOBytes",     "FileSystemId", fs_id, { "stat":"Sum",     "period":60,  "id":"mi_${local.efs_id_to_token[fs_id]}" } ],
      [ "AWS/EFS", "PermittedThroughput","FileSystemId", fs_id, { "stat":"Average", "period":60,  "id":"pt_${local.efs_id_to_token[fs_id]}" } ],
      [ { "expression":"(mi_${local.efs_id_to_token[fs_id]}/60)/pt_${local.efs_id_to_token[fs_id]}*100", "id":"tp_${local.efs_id_to_token[fs_id]}", "label":"${fs_id}-FileServerDiskThroughputUtilization" } ],
      [ "AWS/EFS", "StorageBytes",       "FileSystemId", fs_id, { "stat":"Average", "period":300, "id":"sb_${local.efs_id_to_token[fs_id]}" } ],
      [ { "expression":"sb_${local.efs_id_to_token[fs_id]} / ${var.efs_target_capacity_bytes} * 100",   "id":"su_${local.efs_id_to_token[fs_id]}", "label":"${fs_id}-StorageCapacityUtilization" } ]
    ]
  ])
}
